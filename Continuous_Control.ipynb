{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInvalid requirement: './python'\n",
      "It looks like a path. File './python' does not exist.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p2_continuous-control/Reacher_Linux/Reacher.x86_64')\n",
    "                      # /data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the default brain\n",
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]\n",
    "\n",
    "# print(\"Action space type:\",brain.vector_action_space_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the environment\n",
    "# env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# # number of agents\n",
    "# num_agents = len(env_info.agents)\n",
    "# print('Number of agents:', num_agents)\n",
    "\n",
    "# # size of each action\n",
    "# action_size = brain.vector_action_space_size\n",
    "# print('Size of each action:', action_size)\n",
    "\n",
    "# # examine the state space \n",
    "# states = env_info.vector_observations\n",
    "# state_size = states.shape[1]\n",
    "# print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "# print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_file(name, txt,mode):\n",
    "    with open(name + '.txt', mode) as file:  # 'file.txt'\n",
    "        file.write(str(txt) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        #self.load_state_dict(torch.load('checkpoint_actor.pth', map_location='cpu'))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "        #self.load_state_dict(torch.load('checkpoint_critic.pth', map_location='cpu'))                \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.bn1(self.fcs1(state)))\n",
    "        \n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "#from model import Actor, Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0000     # L2 weight decay\n",
    "BATCH_SIZE = 1024         # minibatch size\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def start_learn(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        #print( Q_targets_next.shape , rewards.shape , dones.shape )\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.randn() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='/home/deeplearning/Desktop/RL/deep-reinforcement-learning/p2_continuous-control/Reacher_Linux/Reacher.x86_64')\n",
    "\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "#env.seed(2)\n",
    "agent = Agent(state_size=33, action_size=4, random_seed=2)\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:105: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.33\n",
      "Episode ended in :182.328360080719 secs\n",
      "Episode 2\tAverage Score: 0.45\n",
      "Episode ended in :199.09508848190308 secs\n",
      "Episode 3\tAverage Score: 0.57\n",
      "Episode ended in :196.0718216896057 secs\n",
      "Episode 4\tAverage Score: 0.68\n",
      "Episode ended in :207.35248613357544 secs\n",
      "Episode 5\tAverage Score: 0.86\n",
      "Episode ended in :202.69183015823364 secs\n",
      "Episode 6\tAverage Score: 0.99\n",
      "Episode ended in :216.26951122283936 secs\n",
      "Episode 7\tAverage Score: 1.20\n",
      "Episode ended in :208.71523308753967 secs\n",
      "Episode 8\tAverage Score: 1.47\n",
      "Episode ended in :199.69709420204163 secs\n",
      "Episode 9\tAverage Score: 1.68\n",
      "Episode ended in :201.9520025253296 secs\n",
      "Episode 10\tAverage Score: 1.96\n",
      "Episode ended in :204.81691908836365 secs\n",
      "Episode 11\tAverage Score: 2.26\n",
      "Episode ended in :210.75218176841736 secs\n",
      "Episode 12\tAverage Score: 2.63\n",
      "Episode ended in :208.36474871635437 secs\n",
      "Episode 13\tAverage Score: 3.02\n",
      "Episode ended in :214.00379705429077 secs\n",
      "Episode 14\tAverage Score: 3.48\n",
      "Episode ended in :215.9468731880188 secs\n",
      "Episode 15\tAverage Score: 4.11\n",
      "Episode ended in :233.79549980163574 secs\n",
      "Episode 16\tAverage Score: 4.78\n",
      "Episode ended in :228.9897108078003 secs\n",
      "Episode 17\tAverage Score: 5.53\n",
      "Episode ended in :221.8574993610382 secs\n",
      "Episode 18\tAverage Score: 6.40\n",
      "Episode ended in :229.38498163223267 secs\n",
      "Episode 19\tAverage Score: 7.32\n",
      "Episode ended in :254.47681736946106 secs\n",
      "Episode 20\tAverage Score: 8.18\n",
      "Episode ended in :253.15132927894592 secs\n",
      "Episode 21\tAverage Score: 9.22\n",
      "Episode ended in :273.0177729129791 secs\n",
      "Episode 22\tAverage Score: 10.20\n",
      "Episode ended in :278.4421579837799 secs\n",
      "Episode 23\tAverage Score: 11.16\n",
      "Episode ended in :275.8742792606354 secs\n",
      "Episode 24\tAverage Score: 12.11\n",
      "Episode ended in :362.7902579307556 secs\n",
      "Episode 25\tAverage Score: 12.98\n",
      "Episode ended in :130.22458362579346 secs\n",
      "Episode 26\tAverage Score: 13.80\n",
      "Episode ended in :130.5371482372284 secs\n",
      "Episode 27\tAverage Score: 14.61\n",
      "Episode ended in :130.38234210014343 secs\n",
      "Episode 28\tAverage Score: 15.40\n",
      "Episode ended in :136.9378776550293 secs\n",
      "Episode 29\tAverage Score: 16.17\n",
      "Episode ended in :154.9835705757141 secs\n",
      "Episode 30\tAverage Score: 16.80\n",
      "Episode ended in :162.67844080924988 secs\n",
      "Episode 31\tAverage Score: 17.40\n",
      "Episode ended in :157.63554549217224 secs\n",
      "Episode 32\tAverage Score: 18.03\n",
      "Episode ended in :147.03164267539978 secs\n",
      "Episode 33\tAverage Score: 18.61\n",
      "Episode ended in :140.45353841781616 secs\n",
      "Episode 34\tAverage Score: 19.15\n",
      "Episode ended in :182.14148545265198 secs\n",
      "Episode 35\tAverage Score: 19.65\n",
      "Episode ended in :170.83024406433105 secs\n",
      "Episode 36\tAverage Score: 20.12\n",
      "Episode ended in :173.72451639175415 secs\n",
      "Episode 37\tAverage Score: 20.58\n",
      "Episode ended in :170.58980655670166 secs\n",
      "Episode 38\tAverage Score: 20.99\n",
      "Episode ended in :157.18663835525513 secs\n",
      "Episode 39\tAverage Score: 21.37\n",
      "Episode ended in :163.46459031105042 secs\n",
      "Episode 40\tAverage Score: 21.75\n",
      "Episode ended in :164.80589032173157 secs\n",
      "Episode 41\tAverage Score: 22.11\n",
      "Episode ended in :165.3992898464203 secs\n",
      "Episode 42\tAverage Score: 22.45\n",
      "Episode ended in :158.81367444992065 secs\n",
      "Episode 43\tAverage Score: 22.79\n",
      "Episode ended in :179.95923280715942 secs\n",
      "Episode 44\tAverage Score: 23.10\n",
      "Episode ended in :173.8290891647339 secs\n",
      "Episode 45\tAverage Score: 23.41\n",
      "Episode ended in :172.60904574394226 secs\n",
      "Episode 46\tAverage Score: 23.69\n",
      "Episode ended in :210.83868169784546 secs\n",
      "Episode 47\tAverage Score: 23.99\n",
      "Episode ended in :232.9696319103241 secs\n",
      "Episode 48\tAverage Score: 24.28\n",
      "Episode ended in :233.1008973121643 secs\n",
      "Episode 49\tAverage Score: 24.56\n",
      "Episode ended in :239.98249769210815 secs\n",
      "Episode 50\tAverage Score: 24.80\n",
      "Episode ended in :229.87331318855286 secs\n",
      "Episode 51\tAverage Score: 25.05\n",
      "Episode ended in :186.01521229743958 secs\n",
      "Episode 52\tAverage Score: 25.30\n",
      "Episode ended in :181.78593683242798 secs\n",
      "Episode 53\tAverage Score: 25.54\n",
      "Episode ended in :202.55042147636414 secs\n",
      "Episode 54\tAverage Score: 25.76\n",
      "Episode ended in :225.0772476196289 secs\n",
      "Episode 55\tAverage Score: 25.99\n",
      "Episode ended in :197.85489749908447 secs\n",
      "Episode 56\tAverage Score: 26.21\n",
      "Episode ended in :186.01089787483215 secs\n",
      "Episode 57\tAverage Score: 26.42\n",
      "Episode ended in :187.77013206481934 secs\n",
      "Episode 58\tAverage Score: 26.61\n",
      "Episode ended in :190.3013472557068 secs\n",
      "Episode 59\tAverage Score: 26.80\n",
      "Episode ended in :184.7496588230133 secs\n",
      "Episode 60\tAverage Score: 26.98\n",
      "Episode ended in :183.56217408180237 secs\n",
      "Episode 61\tAverage Score: 27.16\n",
      "Episode ended in :173.21405816078186 secs\n",
      "Episode 62\tAverage Score: 27.34\n",
      "Episode ended in :166.50200724601746 secs\n",
      "Episode 63\tAverage Score: 27.51\n",
      "Episode ended in :171.61011791229248 secs\n",
      "Episode 64\tAverage Score: 27.67\n",
      "Episode ended in :189.0171947479248 secs\n",
      "Episode 65\tAverage Score: 27.84\n",
      "Episode ended in :182.3506407737732 secs\n",
      "Episode 66\tAverage Score: 28.01\n",
      "Episode ended in :188.49178528785706 secs\n",
      "Episode 67\tAverage Score: 28.17\n",
      "Episode ended in :179.05639839172363 secs\n",
      "Episode 68\tAverage Score: 28.32\n",
      "Episode ended in :179.20711994171143 secs\n",
      "Episode 69\tAverage Score: 28.48\n",
      "Episode ended in :175.95538640022278 secs\n",
      "Episode 70\tAverage Score: 28.61\n",
      "Episode ended in :176.21549201011658 secs\n",
      "Episode 71\tAverage Score: 28.75\n",
      "Episode ended in :172.22936272621155 secs\n",
      "Episode 72\tAverage Score: 28.88\n",
      "Episode ended in :173.59647226333618 secs\n",
      "Episode 73\tAverage Score: 29.02\n",
      "Episode ended in :183.19614934921265 secs\n",
      "Episode 74\tAverage Score: 29.14\n",
      "Episode ended in :180.68439626693726 secs\n",
      "Episode 75\tAverage Score: 29.27\n",
      "Episode ended in :179.07555294036865 secs\n",
      "Episode 76\tAverage Score: 29.37\n",
      "Episode ended in :177.31075978279114 secs\n",
      "Episode 77\tAverage Score: 29.49\n",
      "Episode ended in :170.56420397758484 secs\n",
      "Episode 78\tAverage Score: 29.59\n",
      "Episode ended in :172.35886478424072 secs\n",
      "Episode 79\tAverage Score: 29.69\n",
      "Episode ended in :167.22450995445251 secs\n",
      "Episode 80\tAverage Score: 29.79\n",
      "Episode ended in :170.46816754341125 secs\n",
      "Episode 81\tAverage Score: 29.89\n",
      "Episode ended in :168.53243565559387 secs\n",
      "Episode 82\tAverage Score: 29.98\n",
      "Episode ended in :170.07019352912903 secs\n",
      "Episode 83\tAverage Score: 30.08\n",
      "Episode ended in :172.22720170021057 secs\n",
      "Episode 84\tAverage Score: 30.18\n",
      "Episode ended in :167.2782759666443 secs\n",
      "Episode 85\tAverage Score: 30.28\n",
      "Episode ended in :196.22336649894714 secs\n",
      "Episode 86\tAverage Score: 30.37\n",
      "Episode ended in :202.51961088180542 secs\n",
      "Episode 87\tAverage Score: 30.46\n",
      "Episode ended in :190.92465996742249 secs\n",
      "Episode 88\tAverage Score: 30.55\n",
      "Episode ended in :180.57316493988037 secs\n",
      "Episode 89\tAverage Score: 30.64\n",
      "Episode ended in :194.77233862876892 secs\n",
      "Episode 90\tAverage Score: 30.72\n",
      "Episode ended in :213.42610549926758 secs\n",
      "Episode 91\tAverage Score: 30.79\n",
      "Episode ended in :188.71467089653015 secs\n",
      "Episode 92\tAverage Score: 30.86\n",
      "Episode ended in :185.97920083999634 secs\n",
      "Episode 93\tAverage Score: 30.94\n",
      "Episode ended in :176.554678440094 secs\n",
      "Episode 94\tAverage Score: 31.03\n",
      "Episode ended in :248.57237029075623 secs\n",
      "Episode 95\tAverage Score: 31.11\n",
      "Episode ended in :180.2245512008667 secs\n",
      "Episode 96\tAverage Score: 31.19\n",
      "Episode ended in :177.53402423858643 secs\n",
      "Episode 97\tAverage Score: 31.26\n",
      "Episode ended in :182.73730373382568 secs\n",
      "Episode 98\tAverage Score: 31.34\n",
      "Episode ended in :180.10726714134216 secs\n",
      "Episode 99\tAverage Score: 31.40\n",
      "Episode ended in :185.90333652496338 secs\n",
      "Episode 100\tAverage Score: 31.47\n",
      "Episode ended in :182.1100196838379 secs\n",
      "Episode 101\tAverage Score: 31.54\n",
      "\n",
      "Environment solved in 101 episodes!\tAverage Score: 31.540\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2a28e3013bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2a28e3013bd6>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mlog_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "num_agents=20\n",
    "\n",
    "from time import strftime,localtime\n",
    "\n",
    "solved_score = 30\n",
    "\n",
    "name = \"ddpg_cont_control\"+\"_\"+strftime(\"%Y-%m-%d_%H-%M-%S\", localtime())\n",
    "\n",
    "\n",
    "def ddpg(n_episodes=2000, max_t=1000, print_every=1000):\n",
    "    total_scores_deque = deque(maxlen=print_every)\n",
    "    total_scores = []\n",
    "   \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        start = time.time()\n",
    "        txt = \"\"\n",
    "        #state = env.reset()\n",
    "        #print(type(state['ReacherBrain'].vector_observations))\n",
    "        #states = state['ReacherBrain'].vector_observations\n",
    "        agent.reset()\n",
    "        #score = 0\n",
    "        txt = txt+\"Episode: \"+str(i_episode)+'\\n\\n'\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        score = np.zeros(num_agents)     \n",
    "        for t in range(max_t):\n",
    "#         t=0\n",
    "#         while True:\n",
    "            #t=t+1\n",
    "            #print('\\rEpisode {}\\tTime Step: {}'.format(i_episode, t, end=\"\"))\n",
    "            txt = txt+\"Time step: \"+str(t)+'\\n\\n'\n",
    "            actions = agent.act(states)\n",
    "            txt = txt+\"Action values\" +str(actions)+'\\n'\n",
    "            #print(str(action))\n",
    "            #action = np.clip(action, -1, 1)\n",
    "            \n",
    "#             print(\"env.step(action): \",type(env.step(action)[brain_name]))    \n",
    "#             next_state, reward, done, _ = env.step(action)[brain_name]\n",
    "#             agent.step(state, action, reward, next_state, done)\n",
    "#             state = next_state\n",
    "#             score += reward\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            #print (env_info['ReacherBrain'].vector_observations)\n",
    "            \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                        # get reward (for each agent)\n",
    "            #print(\"Rewards: \"+str(env_info['ReacherBrain'].rewards))\n",
    "            if rewards[0]>0:\n",
    "                txt = txt+\"Reward: \" +str(rewards)+'\\n'\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            \n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done) # send actions to the agent\n",
    "            \n",
    "            #scores += env_info.rewards                           # update the score (for each agent)\n",
    "            #states = next_states                                # roll over states to next time step\n",
    "    \n",
    "            #agent.step(states, action, rewards, next_states, dones)\n",
    "            \n",
    "            score += rewards                         # update the score (for each agent)\n",
    "            states = next_states\n",
    "            if t%10 == 0:\n",
    "                for _ in range(10):\n",
    "                    agent.start_learn()\n",
    "            \n",
    "            if dones[0]==1:\n",
    "                txt = txt+\"Is Done: \" +str(dones)+'\\n'\n",
    "          \n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        mean_score = np.mean(score)\n",
    "        #min_score = np.min(scores)\n",
    "        #max_score = np.max(scores)\n",
    "        total_scores_deque.append(mean_score)\n",
    "        total_scores.append(mean_score)\n",
    "        total_average_score = np.mean(total_scores_deque)\n",
    "        \n",
    "        # log data\n",
    "        log_file(name, txt,'a')    \n",
    "        #average_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, total_average_score, end=\"\"))\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, total_average_score))\n",
    "        \n",
    "        if i_episode > 100 and total_average_score >= solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, total_average_score))\n",
    "\n",
    "            # Save the recorded Scores data\n",
    "            scores_filename = \"ddpgAgent_Scores.csv\"\n",
    "            np.savetxt(scores_filename, total_scores, delimiter=\",\")\n",
    "            break \n",
    "        end = time.time()\n",
    "        episodeTime = end - start      \n",
    "        txt = \"Episode ended in :\"+  str(episodeTime) + \" secs\"\n",
    "        print(txt)\n",
    "        log_file(name, txt,'a')      \n",
    "    return total_scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved in 101 episodes!\tAverage Score: 31.540\n"
     ]
    }
   ],
   "source": [
    "print('Environment solved in 101 episodes!\tAverage Score: 31.540')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "total_scores\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f000fdafe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f000daafdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(states))\n",
    "print('total_scores')\n",
    "\n",
    "scores_filename = \"ddpgAgent_Scores.csv\"\n",
    "#np.savetxt(scores_filename, total_scores, delimiter=\",\")\n",
    "\n",
    "import pandas as pd\n",
    "scores = pd.read_csv(scores_filename, header=None , names=['Score'])\n",
    "scores\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('Average_score_over100_episodes_v1.png')\n",
    "\n",
    "#import csv\n",
    "\n",
    "#with open(scores_filename, mode='r') as file1:\n",
    "#    employee_writer = csv.read(file1, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
